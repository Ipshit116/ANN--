{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T18:33:01.023174Z",
     "iopub.status.busy": "2023-02-18T18:33:01.022788Z",
     "iopub.status.idle": "2023-02-18T18:33:07.932348Z",
     "shell.execute_reply": "2023-02-18T18:33:07.931322Z",
     "shell.execute_reply.started": "2023-02-18T18:33:01.023142Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Set the same memory growth setting for all GPUs.\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T18:33:11.485916Z",
     "iopub.status.busy": "2023-02-18T18:33:11.485284Z",
     "iopub.status.idle": "2023-02-18T18:33:13.985394Z",
     "shell.execute_reply": "2023-02-18T18:33:13.984366Z",
     "shell.execute_reply.started": "2023-02-18T18:33:11.485881Z"
    },
    "id": "0_NRS5C-Ic8D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ProgbarLogger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import seaborn as sns\n",
    "import joblib # Saves and loads the model to save training time\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import seaborn as sns\n",
    "import joblib # Saves and loads the model to save training time\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T18:33:23.603131Z",
     "iopub.status.busy": "2023-02-18T18:33:23.602742Z",
     "iopub.status.idle": "2023-02-18T18:33:23.616443Z",
     "shell.execute_reply": "2023-02-18T18:33:23.614978Z",
     "shell.execute_reply.started": "2023-02-18T18:33:23.603098Z"
    },
    "id": "yrWMV1AJJVZg",
    "outputId": "3a7a72b7-83e6-4d8c-97b3-e6befa48df1d"
   },
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T18:33:30.901738Z",
     "iopub.status.busy": "2023-02-18T18:33:30.901364Z",
     "iopub.status.idle": "2023-02-18T18:33:32.207761Z",
     "shell.execute_reply": "2023-02-18T18:33:32.206693Z",
     "shell.execute_reply.started": "2023-02-18T18:33:30.901707Z"
    },
    "id": "RW6_9sRxJVcs",
    "outputId": "a9cf8ec1-e775-48df-e008-21cc1476bf21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                  0\n",
      "Surface Type        0\n",
      "Material            0\n",
      "Color               0\n",
      "Manufacturer        0\n",
      "Bead Type 1         0\n",
      "Bead Type 2         0\n",
      "Interval            0\n",
      "Traffic             0\n",
      "Rainfall            0\n",
      "Snowfall            0\n",
      "MRS             14224\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Surface Type</th>\n",
       "      <th>Material</th>\n",
       "      <th>Color</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Bead Type 1</th>\n",
       "      <th>Bead Type 2</th>\n",
       "      <th>Interval</th>\n",
       "      <th>Traffic</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Snowfall</th>\n",
       "      <th>MRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMM-2012-01-00161010834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>86665.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMM-2012-01-00162010834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>86665.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMM-2012-01-00181010834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>86665.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMM-2012-01-00182010834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>86665.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMM-2012-01-001629110834</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>86665.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID  Surface Type  Material  Color  Manufacturer  \\\n",
       "0   PMM-2012-01-00161010834             0         1      0             8   \n",
       "1   PMM-2012-01-00162010834             0         1      0             8   \n",
       "2   PMM-2012-01-00181010834             0         1      0             8   \n",
       "3   PMM-2012-01-00182010834             0         1      0             8   \n",
       "4  PMM-2012-01-001629110834             1         1      0             8   \n",
       "\n",
       "   Bead Type 1  Bead Type 2  Interval  Traffic  Rainfall  Snowfall    MRS  \n",
       "0            3            4         0  86665.0      1.49       0.0  441.0  \n",
       "1            3            4         0  86665.0      1.49       0.0  642.0  \n",
       "2            3            4         0  86665.0      1.49       0.0  462.0  \n",
       "3            3            4         0  86665.0      1.49       0.0  481.0  \n",
       "4            3            4         0  86665.0      1.49       0.0  579.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from google.colab import files\n",
    "import pandas as pd\n",
    "\n",
    "# upload the data file to Google Colab Pro\n",
    "#uploaded = files.upload()\n",
    "\n",
    "# read the data file into a Pandas DataFrame\n",
    "df = pd.read_csv('Combined_data.csv', keep_default_na=False, na_values=[''])\n",
    "\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "df[\"Surface Type\"]=label_encoder.fit_transform(df[\"Surface Type\"])\n",
    "df[\"Material\"]=label_encoder.fit_transform(df[\"Material\"])\n",
    "df[\"Color\"]=label_encoder.fit_transform(df[\"Color\"])\n",
    "df[\"Manufacturer\"]=label_encoder.fit_transform(df[\"Manufacturer\"])\n",
    "df[\"Bead Type 1\"]=label_encoder.fit_transform(df[\"Bead Type 1\"])\n",
    "df[\"Bead Type 2\"]=label_encoder.fit_transform(df[\"Bead Type 2\"])\n",
    "\n",
    "df.to_csv('Combined_data_encoded_ANN.csv',index=False)\n",
    "df = pd.read_csv('Combined_data_encoded_ANN.csv',keep_default_na=False,na_values=[''])\n",
    "\n",
    "df_2 = df.copy()\n",
    "\n",
    "df_2 = df_2.astype({'NTPEP Number':'string', \n",
    "                'Sub-Deck':'string',\n",
    "                'Line':'string',\n",
    "                'Surface Type':'string',\n",
    "                'Color':'string',\n",
    "                'Material':'string',\n",
    "                'Manufacturer':'string',\n",
    "                #'Thickness':'string',\n",
    "                'Bead Type 1':'string',\n",
    "                'Bead Type 2':'string'\n",
    "                })\n",
    "\n",
    "\n",
    "df_2[\"ID\"] = df_2['NTPEP Number']+df_2['Sub-Deck']+df_2['Line']+df_2['Surface Type']+df_2['Material']+df_2['Color']+df_2['Manufacturer']+df_2['Bead Type 1']+df_2['Bead Type 2']\n",
    "#df_2['Thickness']\n",
    "\n",
    "\n",
    "df[\"ID\"]=df_2[\"ID\"]\n",
    "\n",
    "df.drop(columns=[\"Site\",\"NTPEP Number\",\"Sub-Deck\",\"Line\"],axis=1,inplace=True)\n",
    "\n",
    "df = df [['ID', 'Surface Type', 'Material', 'Color', 'Manufacturer',\n",
    "       #'Thickness', \n",
    "        'Bead Type 1', 'Bead Type 2', 'Interval', 'Traffic',\n",
    "       'Rainfall', 'Snowfall', 'MRS']]\n",
    "\n",
    "df.to_csv('Combined_data_encoded_with_ID_ANN.csv',index=False)\n",
    "\n",
    "\n",
    "print(df.isnull().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJloo-D9d-OU"
   },
   "source": [
    "**Creating a function to make ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T18:33:40.798206Z",
     "iopub.status.busy": "2023-02-18T18:33:40.797827Z",
     "iopub.status.idle": "2023-02-18T18:33:40.807377Z",
     "shell.execute_reply": "2023-02-18T18:33:40.806031Z",
     "shell.execute_reply.started": "2023-02-18T18:33:40.798174Z"
    },
    "id": "p6_mFF_3aMmN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model creation done\n"
     ]
    }
   ],
   "source": [
    "def create_model(hidden_layers=1, nodes=16, dropout=0.0, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nodes, input_dim=X_train.shape[1], activation='relu'))\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(Dense(nodes, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# Create the Keras wrapper for use in GridSearchCV\n",
    "model = KerasRegressor(build_fn=create_model, verbose=10)\n",
    "\n",
    "print('Model creation done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz6rDvu9d512"
   },
   "source": [
    "**Model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T18:36:51.040891Z",
     "iopub.status.busy": "2023-02-18T18:36:51.040512Z",
     "iopub.status.idle": "2023-02-18T20:15:39.542815Z",
     "shell.execute_reply": "2023-02-18T20:15:39.541446Z",
     "shell.execute_reply.started": "2023-02-18T18:36:51.040860Z"
    },
    "id": "e8TrDMiTJVUN",
    "outputId": "00c65a76-ceba-46e3-abc0-1a511a73524b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "ANN - Model 1 - Number of points (total): 5316\n",
      "ANN - Model 1 - Number of points (train): 4252\n",
      "ANN - Model 1 - Number of points (test): 1064\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "[CV 1/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 1/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-4986.798 total time=  18.3s\n",
      "[CV 2/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 2/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-9342.248 total time=  24.7s\n",
      "[CV 3/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 3/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-3185.144 total time=  23.6s\n",
      "[CV 4/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 4/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-4860.495 total time=  19.8s\n",
      "[CV 5/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 5/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-6175.855 total time=  20.2s\n",
      "[CV 6/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 6/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-4108.247 total time=  22.8s\n",
      "[CV 7/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 7/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-6274.668 total time=  23.7s\n",
      "[CV 8/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 8/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-6278.973 total time=  29.3s\n",
      "[CV 9/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 9/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-4240.301 total time=  41.5s\n",
      "[CV 10/10; 1/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300\n",
      "[CV 10/10; 1/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=300;, score=-5227.039 total time=  42.8s\n",
      "[CV 1/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 1/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-5267.468 total time=  32.8s\n",
      "[CV 2/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 2/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-9503.890 total time=  27.7s\n",
      "[CV 3/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 3/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-3951.328 total time=  17.6s\n",
      "[CV 4/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 4/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-4901.620 total time=  16.3s\n",
      "[CV 5/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 5/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-6324.899 total time=  21.7s\n",
      "[CV 6/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 6/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-4227.545 total time=  27.3s\n",
      "[CV 7/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 7/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-7686.535 total time=  26.5s\n",
      "[CV 8/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 8/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-9074.073 total time=  28.2s\n",
      "[CV 9/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 9/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-4309.200 total time= 1.2min\n",
      "[CV 10/10; 2/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400\n",
      "[CV 10/10; 2/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=400;, score=-5411.144 total time= 1.1min\n",
      "[CV 1/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 1/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-4200.965 total time=  52.2s\n",
      "[CV 2/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 2/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-12207.865 total time=  24.8s\n",
      "[CV 3/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 3/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-4574.382 total time=  20.8s\n",
      "[CV 4/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 4/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-4674.080 total time=  34.8s\n",
      "[CV 5/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 5/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-6261.782 total time=  28.7s\n",
      "[CV 6/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 6/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-5208.921 total time=  30.1s\n",
      "[CV 7/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 7/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-6054.635 total time=  48.0s\n",
      "[CV 8/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 8/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-5427.302 total time= 1.1min\n",
      "[CV 9/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 9/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-4187.253 total time= 1.1min\n",
      "[CV 10/10; 3/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500\n",
      "[CV 10/10; 3/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.001, nodes=500;, score=-4894.543 total time= 1.2min\n",
      "[CV 1/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 1/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-6533.605 total time=   8.7s\n",
      "[CV 2/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-11247.571 total time=   7.0s\n",
      "[CV 3/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 3/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-5241.628 total time=   5.6s\n",
      "[CV 4/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 4/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-5015.678 total time=  10.2s\n",
      "[CV 5/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 5/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-7242.607 total time=   5.1s\n",
      "[CV 6/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 6/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-5224.935 total time=   9.2s\n",
      "[CV 7/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 7/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-8930.159 total time=   7.2s\n",
      "[CV 8/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 8/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-10009.144 total time=   5.7s\n",
      "[CV 9/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 9/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-4288.791 total time=  12.6s\n",
      "[CV 10/10; 4/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300\n",
      "[CV 10/10; 4/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=300;, score=-7996.066 total time=   8.7s\n",
      "[CV 1/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 1/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-6598.438 total time=   8.7s\n",
      "[CV 2/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 2/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-10494.016 total time=   9.8s\n",
      "[CV 3/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 3/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-4664.436 total time=  10.1s\n",
      "[CV 4/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 4/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-5307.508 total time=   9.1s\n",
      "[CV 5/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 5/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-6573.502 total time=   9.5s\n",
      "[CV 6/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 6/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-5957.115 total time=   6.1s\n",
      "[CV 7/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 7/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-9391.635 total time=   7.4s\n",
      "[CV 8/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 8/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-9214.249 total time=  12.0s\n",
      "[CV 9/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 9/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-4955.941 total time=  13.4s\n",
      "[CV 10/10; 5/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400\n",
      "[CV 10/10; 5/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=400;, score=-8555.102 total time=   9.2s\n",
      "[CV 1/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 1/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-7004.643 total time=   7.6s\n",
      "[CV 2/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 2/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-10829.947 total time=  10.5s\n",
      "[CV 3/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 3/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-5302.998 total time=   6.9s\n",
      "[CV 4/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 4/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-4798.948 total time=  13.6s\n",
      "[CV 5/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 5/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-7378.780 total time=   6.7s\n",
      "[CV 6/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 6/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-5895.598 total time=   7.0s\n",
      "[CV 7/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 7/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-9898.150 total time=   7.2s\n",
      "[CV 8/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 8/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-8513.081 total time=  11.6s\n",
      "[CV 9/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 9/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-7574.165 total time=   4.6s\n",
      "[CV 10/10; 6/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500\n",
      "[CV 10/10; 6/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.01, nodes=500;, score=-9060.212 total time=  13.4s\n",
      "[CV 1/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 1/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-8894.339 total time=   3.5s\n",
      "[CV 2/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 2/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-17655.674 total time=   3.0s\n",
      "[CV 3/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 3/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-10539.263 total time=   2.0s\n",
      "[CV 4/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-13614.408 total time=   1.9s\n",
      "[CV 5/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 5/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-6877.752 total time=   4.3s\n",
      "[CV 6/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 6/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-9297.170 total time=   2.4s\n",
      "[CV 7/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 7/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-12964.396 total time=   2.6s\n",
      "[CV 8/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 8/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-9933.755 total time=   4.6s\n",
      "[CV 9/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 9/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-6406.813 total time=   4.4s\n",
      "[CV 10/10; 7/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300\n",
      "[CV 10/10; 7/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=300;, score=-10115.183 total time=   4.3s\n",
      "[CV 1/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 1/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-5226.499 total time=   5.3s\n",
      "[CV 2/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 2/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-8682.910 total time=   4.7s\n",
      "[CV 3/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 3/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-9797.974 total time=   2.5s\n",
      "[CV 4/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 4/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-5908.721 total time=   5.8s\n",
      "[CV 5/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 5/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-7701.695 total time=   4.5s\n",
      "[CV 6/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 6/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-6940.421 total time=   2.8s\n",
      "[CV 7/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 7/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-12800.840 total time=   3.4s\n",
      "[CV 8/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 8/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-9879.102 total time=   5.8s\n",
      "[CV 9/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 9/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-6497.670 total time=   7.1s\n",
      "[CV 10/10; 8/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400\n",
      "[CV 10/10; 8/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=400;, score=-9124.714 total time=   6.6s\n",
      "[CV 1/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 1/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-5856.763 total time=   6.9s\n",
      "[CV 2/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 2/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-88869.265 total time= 1.5min\n",
      "[CV 3/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 3/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-67460.028 total time= 1.4min\n",
      "[CV 4/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 4/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-4862.249 total time=   7.0s\n",
      "[CV 5/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 5/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-9634.890 total time=   3.6s\n",
      "[CV 6/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 6/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-4336.787 total time=   8.1s\n",
      "[CV 7/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 7/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-6895.945 total time=   6.0s\n",
      "[CV 8/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 8/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-6096.465 total time=   7.1s\n",
      "[CV 9/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 9/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-5556.133 total time=   6.6s\n",
      "[CV 10/10; 9/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500\n",
      "[CV 10/10; 9/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=1, learning_rate=0.1, nodes=500;, score=-7208.136 total time=   6.7s\n",
      "[CV 1/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 1/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-4842.888 total time=  19.8s\n",
      "[CV 2/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 2/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-7737.997 total time=  22.5s\n",
      "[CV 3/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 3/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-3282.374 total time=  17.8s\n",
      "[CV 4/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 4/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-4826.609 total time=  17.6s\n",
      "[CV 5/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 5/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-5485.585 total time=  17.8s\n",
      "[CV 6/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-3929.019 total time=  18.7s\n",
      "[CV 7/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 7/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-6087.213 total time=  18.9s\n",
      "[CV 8/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 8/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-4575.880 total time=  25.2s\n",
      "[CV 9/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 9/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-4224.953 total time=  24.5s\n",
      "[CV 10/10; 10/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300\n",
      "[CV 10/10; 10/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=300;, score=-4352.484 total time=  30.2s\n",
      "[CV 1/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 1/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-4932.306 total time=  21.1s\n",
      "[CV 2/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 2/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-6855.628 total time=  32.1s\n",
      "[CV 3/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 3/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-3245.781 total time=  17.6s\n",
      "[CV 4/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 4/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-4323.741 total time=  18.4s\n",
      "[CV 5/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 5/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-5457.296 total time=  20.4s\n",
      "[CV 6/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 6/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-4055.551 total time=  27.2s\n",
      "[CV 7/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 7/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-5671.422 total time=  34.8s\n",
      "[CV 8/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 8/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-4414.382 total time=  53.3s\n",
      "[CV 9/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 9/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-4501.172 total time=  38.1s\n",
      "[CV 10/10; 11/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400\n",
      "[CV 10/10; 11/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=400;, score=-5014.471 total time=  38.7s\n",
      "[CV 1/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 1/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-4485.905 total time=  32.2s\n",
      "[CV 2/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 2/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-8962.056 total time=  37.7s\n",
      "[CV 3/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 3/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-3147.369 total time=  27.4s\n",
      "[CV 4/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 4/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-4314.988 total time=  22.8s\n",
      "[CV 5/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 5/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-4711.974 total time=  32.2s\n",
      "[CV 6/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 6/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-3960.792 total time=  31.6s\n",
      "[CV 7/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 7/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-5804.861 total time=  34.7s\n",
      "[CV 8/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 8/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-5762.829 total time=  29.7s\n",
      "[CV 9/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 9/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-4791.140 total time=  42.0s\n",
      "[CV 10/10; 12/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500\n",
      "[CV 10/10; 12/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.001, nodes=500;, score=-4296.050 total time=  35.1s\n",
      "[CV 1/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 1/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-6069.732 total time=   9.2s\n",
      "[CV 2/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 2/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-7802.294 total time=  17.6s\n",
      "[CV 3/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 3/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-5678.836 total time=   5.2s\n",
      "[CV 4/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 4/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-5746.524 total time=  10.4s\n",
      "[CV 5/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 5/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-6409.503 total time=   6.8s\n",
      "[CV 6/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 6/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-5675.847 total time=  10.4s\n",
      "[CV 7/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 7/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-7163.830 total time=   9.1s\n",
      "[CV 8/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-8019.911 total time=  10.0s\n",
      "[CV 9/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 9/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-5139.933 total time=  11.1s\n",
      "[CV 10/10; 13/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300\n",
      "[CV 10/10; 13/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=300;, score=-10502.229 total time=   6.2s\n",
      "[CV 1/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 1/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-6794.136 total time=  13.5s\n",
      "[CV 2/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 2/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-11149.765 total time=   9.3s\n",
      "[CV 3/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 3/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-4693.622 total time=  10.8s\n",
      "[CV 4/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 4/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-5519.029 total time=  10.6s\n",
      "[CV 5/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 5/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-6453.592 total time=   7.8s\n",
      "[CV 6/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 6/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-4572.784 total time=  10.3s\n",
      "[CV 7/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 7/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-7064.525 total time=  14.0s\n",
      "[CV 8/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 8/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-7405.921 total time=  11.7s\n",
      "[CV 9/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 9/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-6108.380 total time=   9.7s\n",
      "[CV 10/10; 14/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400\n",
      "[CV 10/10; 14/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=400;, score=-9809.185 total time=   8.8s\n",
      "[CV 1/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 1/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-8690.959 total time=   7.6s\n",
      "[CV 2/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 2/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-19175.942 total time=   7.4s\n",
      "[CV 3/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 3/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-9381.441 total time=   6.4s\n",
      "[CV 4/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 4/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-5642.470 total time=  12.5s\n",
      "[CV 5/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 5/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-14485.103 total time=   6.0s\n",
      "[CV 6/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 6/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-4634.003 total time=  12.1s\n",
      "[CV 7/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 7/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-7424.592 total time=  14.8s\n",
      "[CV 8/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 8/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-9363.465 total time=  12.5s\n",
      "[CV 9/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 9/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-5758.553 total time=  10.4s\n",
      "[CV 10/10; 15/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500\n",
      "[CV 10/10; 15/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.01, nodes=500;, score=-8062.391 total time=  10.7s\n",
      "[CV 1/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 1/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-6834.348 total time=   6.1s\n",
      "[CV 2/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 2/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-88912.948 total time=   4.1s\n",
      "[CV 3/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 3/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-67460.690 total time= 1.2min\n",
      "[CV 4/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 4/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-74210.722 total time=   4.1s\n",
      "[CV 5/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 5/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-77324.190 total time=   6.7s\n",
      "[CV 6/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 6/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-58473.840 total time=   6.5s\n",
      "[CV 7/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 7/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-89100.405 total time=   6.0s\n",
      "[CV 8/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 8/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-7997.237 total time=   5.6s\n",
      "[CV 9/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n",
      "[CV 9/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-86405.505 total time=   6.7s\n",
      "[CV 10/10; 16/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10; 16/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=300;, score=-62103.260 total time=   6.0s\n",
      "[CV 1/10; 17/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400\n",
      "[CV 1/10; 17/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400;, score=-78076.907 total time=   9.4s\n",
      "[CV 2/10; 17/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400\n",
      "[CV 2/10; 17/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400;, score=-88834.534 total time=   7.7s\n",
      "[CV 3/10; 17/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400\n",
      "[CV 3/10; 17/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400;, score=-67405.473 total time=   7.7s\n",
      "[CV 4/10; 17/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400\n",
      "[CV 4/10; 17/72] END batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400;, score=-74425.927 total time=   5.6s\n",
      "[CV 5/10; 17/72] START batch_size=128, dropout=0.0, epochs=1000, hidden_layers=2, learning_rate=0.1, nodes=400\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "###### Data processing for the Model 1\n",
    "inputs = df.loc[df['Interval']==0]\n",
    "inputs = inputs.sort_values('ID')\n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall','MRS']]\n",
    "outputs =  df.loc[df['Interval']==1]\n",
    "outputs = outputs.sort_values('ID')\n",
    "y = outputs['MRS']\n",
    "\n",
    "##############################################################\n",
    "print(\"ANN - Model 1 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 1 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 1 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T20:24:46.674435Z",
     "iopub.status.busy": "2023-02-18T20:24:46.674059Z",
     "iopub.status.idle": "2023-02-18T20:24:47.458871Z",
     "shell.execute_reply": "2023-02-18T20:24:47.457853Z",
     "shell.execute_reply.started": "2023-02-18T20:24:46.674405Z"
    }
   },
   "outputs": [],
   "source": [
    "model_1_tuned = grid_result.best_estimator_.model\n",
    "# Saving the model\n",
    "model_1_tuned.save(\"ANN_model_1.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_1_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_1_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_1 = model_1_tuned.predict(X_scaled) ##\n",
    "\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS1\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_1 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_1.to_csv('ANN_Model_1_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS1\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_1 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_1.to_csv('ANN_Model_1_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T20:25:30.376632Z",
     "iopub.status.busy": "2023-02-18T20:25:30.376253Z",
     "iopub.status.idle": "2023-02-18T20:25:37.669031Z",
     "shell.execute_reply": "2023-02-18T20:25:37.668043Z",
     "shell.execute_reply.started": "2023-02-18T20:25:30.376599Z"
    },
    "id": "2UeUiMY6JVPn"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'MRS']\n",
    "explainer = shap.DeepExplainer(model_1_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)\n",
    "\n",
    "#shap.initjs()\n",
    "#shap.force_plot(explainer.expected_value[0],shap_values[0][0],features=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T20:25:44.969145Z",
     "iopub.status.busy": "2023-02-18T20:25:44.968748Z",
     "iopub.status.idle": "2023-02-18T20:25:45.109711Z",
     "shell.execute_reply": "2023-02-18T20:25:45.108719Z",
     "shell.execute_reply.started": "2023-02-18T20:25:44.969111Z"
    },
    "id": "K7m6teCNJVNC"
   },
   "outputs": [],
   "source": [
    "feedrow_model_1 = [[1,4,0,12,0,4,3941418,0.67,0.6,751]]\n",
    "feedrow_model_1 = scaler.transform(feedrow_model_1)\n",
    "predictedoutput_model_1 = model_1_tuned.predict([feedrow_model_1])\n",
    "print(\"Predicted RS at Month 1:\",predictedoutput_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOszUNvmdVz-"
   },
   "source": [
    "**Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T20:29:03.677348Z",
     "iopub.status.busy": "2023-02-18T20:29:03.676869Z"
    },
    "id": "R6WTinZcJVHv"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "###############################################################\n",
    "inputs = df.loc[df['Interval']==1]\n",
    "inputs = inputs.sort_values('ID')\n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1',\n",
    "       'Bead Type 2', 'Traffic','Rainfall','Snowfall']]\n",
    "X['PRS'] = y_pred_1\n",
    "X = X.reset_index()\n",
    "X = X.drop(['index'],axis=1)\n",
    "\n",
    "outputs =  df.loc[df['Interval']==2]\n",
    "outputs = outputs.sort_values('ID')\n",
    "outputs = outputs.reset_index()\n",
    "outputs = outputs.drop(['ID'],axis=1)\n",
    "\n",
    "X['MRS'] = outputs['MRS']\n",
    "null_indexes = X[X['MRS'].isnull()].index.tolist()\n",
    "X = X.dropna()\n",
    "y = X['MRS']\n",
    "X = X.drop(['MRS'],axis=1)\n",
    "##############################################################\n",
    "print(\"ANN - Model 2 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 2 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 2 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_2_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_2_tuned.save(\"ANN_model_2.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_2_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_2_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_2 = model_2_tuned.predict(X_scaled) ##\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_2 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_2= X['PRS'] ####\n",
    "y_pred_2 = y_pred_2.sort_index() ###\n",
    "##########################################################\n",
    "\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS2\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_1 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_1.to_csv('ANN_Model_2_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS2\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_1 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_1.to_csv('ANN_Model_2_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-18T20:28:47.025507Z",
     "iopub.status.idle": "2023-02-18T20:28:47.026123Z",
     "shell.execute_reply": "2023-02-18T20:28:47.025911Z",
     "shell.execute_reply.started": "2023-02-18T20:28:47.025890Z"
    },
    "id": "7uYiZsk_JVE-"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_2_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)\n",
    "\n",
    "#shap.initjs()\n",
    "#shap.force_plot(explainer.expected_value[0],shap_values[0][0],features=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQQhwxTnJVCV"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_2 = [[1,4,0,12,0,4,4162977,0.81,0.6,predictedoutput_model_1]]\n",
    "feedrow_model_2 = scaler.transform(feedrow_model_2)\n",
    "predictedoutput_model_2 = model_2_tuned.predict(feedrow_model_2) ##\n",
    "print(\"Predicted RS at Month 2:\",predictedoutput_model_2)##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f70CNoIdcWN"
   },
   "source": [
    "**Model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7pbea6ebKnc"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==2]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1',\n",
    "       'Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_2.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==3] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 3 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 3 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 3 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_3_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_3_tuned.save(\"ANN_model_3.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_3_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_3_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_3 = model_3_tuned.predict(X_scaled) ##\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_3 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_3 = X['PRS'] ####\n",
    "y_pred_3 = y_pred_3.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS3\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_3 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_3.to_csv('ANN_Model_3_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS3\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_3 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_3.to_csv('ANN_Model_3_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz_W0xk0bKpq"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_3_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iATqnrA1bKsh"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_3 = [[1,4,0,12,0,4,4477824,0.92,0.6,predictedoutput_model_2]]\n",
    "feedrow_model_3 = scaler.transform(feedrow_model_3)\n",
    "predictedoutput_model_3 = model_3_tuned.predict(feedrow_model_3) ##\n",
    "print(\"Predicted RS at Month 3:\",predictedoutput_model_3)##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HraN8iPudjkP"
   },
   "source": [
    "**Model 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUbk4wHhbKxp"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==3]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_3.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==11] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 4 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 4 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 4 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_4_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_4_tuned.save(\"ANN_model_4.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_4_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_4_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_4 = model_4_tuned.predict(X_scaled) ##\n",
    "\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_4 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_4= X['PRS'] ####\n",
    "y_pred_4 = y_pred_4.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS11\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_4 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_4.to_csv('ANN_Model_4_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS11\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_4 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_4.to_csv('ANN_Model_4_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgIpevKybK0I"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_4_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKNOWoY-bK3o"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_4 = [[1,4,0,12,0,4,4897620,0.96,0.6,predictedoutput_model_3]]\n",
    "feedrow_model_4 = scaler.transform(feedrow_model_4)\n",
    "predictedoutput_model_4= model_4_tuned.predict([feedrow_model_4])\n",
    "print(\"Predicted RS at Month 11:\",predictedoutput_model_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxK6h9eRbgt0"
   },
   "source": [
    "**Model 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RE2QPrsbfFQ"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==11]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_4.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==12] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 5 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 5 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 5 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_5_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_5_tuned.save(\"ANN_model_5.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_5_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_5_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_5 = model_5_tuned.predict(X_scaled) ##\n",
    "\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_5 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_5= X['PRS'] ####\n",
    "y_pred_5 = y_pred_5.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS12\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_5 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_5.to_csv('ANN_Model_5_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS12\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_5 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_5.to_csv('ANN_Model_5_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctbVtgLTbfIH"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_5_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwZ9T0-YbfLh"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_5 = [[1,4,0,12,0,4,7684599,1.51,2.2,predictedoutput_model_4]]\n",
    "feedrow_model_5 = scaler.transform(feedrow_model_5)\n",
    "predictedoutput_model_5 = model_5_tuned.predict([feedrow_model_5])\n",
    "print(\"Predicted RS at Month 12:\",predictedoutput_model_5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOM-2Wh-bwkY"
   },
   "source": [
    "**Model 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2okuyLXb0jl"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==12]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_5.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==15] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 6 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 6 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 6 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_6_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_6_tuned.save(\"ANN_model_6.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_6_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_6_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "#Making final prediction\n",
    "y_pred_6 = model_6_tuned.predict(X_scaled) ##\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_6 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_6= X['PRS'] ####\n",
    "y_pred_6 = y_pred_6.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS15\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_6 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_6.to_csv('ANN_Model_6_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS15\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_6 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_6.to_csv('ANN_Model_6_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7NT5TF9b0mP"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_6_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FA5-7Opb0pW"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_6 = [[1,4,0,12,0,4,8174361,1.79,2.2,predictedoutput_model_5]]\n",
    "feedrow_model_6 = scaler.transform(feedrow_model_6)\n",
    "predictedoutput_model_6= model_6_tuned.predict([feedrow_model_6])\n",
    "print(\"Predicted RS at Month 15:\",predictedoutput_model_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VACQeyW_b8qo"
   },
   "source": [
    "**Model 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyzb8aShb0sV"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==15]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_6.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==21] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 7 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 7 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 7 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_7_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_7_tuned.save(\"ANN_model_7.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_7_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_7_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_7 = model_7_tuned.predict(X_scaled) ##\n",
    "\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_7 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_7= X['PRS'] ####\n",
    "y_pred_7 = y_pred_7.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS21\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_7 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_7.to_csv('ANN_Model_7_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS21\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_7 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_7.to_csv('ANN_Model_7_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uc3Kl48Xb0vO"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_7_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uiOHAnYb0x8"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_7 = [[1,4,0,12,0,4,9538698,2.2,2.2,predictedoutput_model_6]]\n",
    "feedrow_model_7 = scaler.transform(feedrow_model_7)\n",
    "predictedoutput_model_7= model_7_tuned.predict([feedrow_model_7])\n",
    "print(\"Predicted RS at Month 21:\",predictedoutput_model_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pe8D_bocE2z"
   },
   "source": [
    "**Model 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAr-A0Frb00y"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==21]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_7.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==24] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 8 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 8 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 8 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_8_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_8_tuned.save(\"ANN_model_8.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_8_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_8_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_8 = model_8_tuned.predict(X_scaled) ##\n",
    "\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_8 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_8= X['PRS'] ####\n",
    "y_pred_8 = y_pred_8.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS24\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_8 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_8.to_csv('ANN_Model_8_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS24\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_8 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_8.to_csv('ANN_Model_8_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLD_ZBC8b04Q"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_8_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-RDRmAEb08H"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_8 = [[1,4,0,12,0,4,11824254,2.74,3.3,predictedoutput_model_7]]\n",
    "feedrow_model_8 = scaler.transform(feedrow_model_8)\n",
    "predictedoutput_model_8= model_8_tuned.predict([feedrow_model_8])\n",
    "print(\"Predicted RS at Month 24:\",predictedoutput_model_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVhRNoi8cPBJ"
   },
   "source": [
    "**Model 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shAsPyPncL_x"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==24]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_8.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==27] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 9 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 9 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 9 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_9_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_9_tuned.save(\"ANN_model_9.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_9_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_9_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "#Making final prediction\n",
    "y_pred_9 = model_9_tuned.predict(X_scaled) ##\n",
    "\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_9 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_9= X['PRS'] ####\n",
    "y_pred_9 = y_pred_8.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS27\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_9 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_9.to_csv('ANN_Model_9_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS27\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_9 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_9.to_csv('ANN_Model_9_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfXmlS0scMCh"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_9_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5e-NbmTcMF9"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_9 = [[1,4,0,12,0,4,12407304,2.86,3.3,predictedoutput_model_8]]\n",
    "feedrow_model_9 = scaler.transform(feedrow_model_9)\n",
    "predictedoutput_model_9= model_9_tuned.predict([feedrow_model_9])\n",
    "print(\"Predicted RS at Month 27:\",predictedoutput_model_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w24CBJ7-cjiI"
   },
   "source": [
    "**Model 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuYHvL3vcMJB"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==27]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_9.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==33] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 10 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 10 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 10 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_10_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_10_tuned.save(\"ANN_model_10.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_10_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_10_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "#Making final prediction\n",
    "y_pred_10 = model_10_tuned.predict(X_scaled) ##\n",
    "\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_10 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_10= X['PRS'] ####\n",
    "y_pred_10 = y_pred_10.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS33\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_10 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_10.to_csv('ANN_Model_10_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS33\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_10 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_10.to_csv('ANN_Model_10_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0TV-XZKcdfZ"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_10_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDM-97JzcsD9"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_10 = [[1,4,0,12,0,4,13864929,3.23,3.3,predictedoutput_model_9]]\n",
    "feedrow_model_10 = scaler.transform(feedrow_model_10)\n",
    "predictedoutput_model_10 = model_10_tuned.predict([feedrow_model_10])\n",
    "print(\"Predicted RS at Month 33:\",predictedoutput_model_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S09xGt3XctPD"
   },
   "source": [
    "**Model 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv7kTQ7Vcwa4"
   },
   "outputs": [],
   "source": [
    "%time \n",
    "####################################################################\n",
    "inputs = df.loc[df['Interval']==33]  \n",
    "inputs = inputs.sort_values('ID') \n",
    "X = inputs[['Surface Type','Material','Color','Manufacturer',#'Thickness',\n",
    "            'Bead Type 1','Bead Type 2', 'Traffic','Rainfall','Snowfall']] \n",
    "\n",
    "X['PRS'] = y_pred_10.values \n",
    "X = X.reset_index() \n",
    "X = X.drop(['index'],axis=1) \n",
    "\n",
    "outputs =  df.loc[df['Interval']==36] # Getting the data from interval 3 (make it 2)\n",
    "outputs = outputs.sort_values('ID')#Sorting wrt UI\n",
    "outputs = outputs.reset_index() # Reseting index\n",
    "outputs = outputs.drop(['index'],axis=1) # dropping the index column\n",
    "   \n",
    "X['MRS'] = outputs['MRS'] # creating a column in X with the measured values from interval 3. This\n",
    "null_indexes1 = X[X['MRS'].isnull()].index.tolist()\n",
    "null_indexes2 = X[X['PRS'].isnull()].index.tolist()\n",
    "# column has NaN\n",
    "#drop nas\n",
    "X = X.dropna() # Dropping NaN from RS column\n",
    "y = X['MRS'] # putting RS from X to y\n",
    "X = X.drop(['MRS'],axis=1) #Dropping RS column from X\n",
    "null_indexes = null_indexes1 + null_indexes2\n",
    "######################################################################\n",
    "print(\"ANN - Model 11 - Number of points (total):\",len(X))## \n",
    "\n",
    "# Splitting data into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "print(\"ANN - Model 11 - Number of points (train):\",len(X_train))##\n",
    "print(\"ANN - Model 11 - Number of points (test):\",len(X_test))##\n",
    "\n",
    "# Scale the predictors using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2],\n",
    "    'nodes': [300, 400, 500],\n",
    "    'dropout': [0.0, 0.05, 0.1,0.2],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size':[128],                  #[32, 64, 128],\n",
    "    'epochs': [1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with 10-fold cross validation\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    verbose=10)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "grid_result = grid.fit(X_train_scaled, \n",
    "                       y_train, \n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [early_stopping, ProgbarLogger()],\n",
    "                       verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding R2 score\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "model_11_tuned = grid_result.best_estimator_.model\n",
    "\n",
    "# Saving the model\n",
    "model_11_tuned.save(\"ANN_model_11.h5\")\n",
    "\n",
    "# Accuracy with the train data\n",
    "y_pred_train_f = model_11_tuned.predict(X_train_scaled)\n",
    "print(\"\\nTrain set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_train, y_pred_train_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_train, y_pred_train_f))\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "adj_r2_train = 1 - (1 - r2_score(y_train, y_pred_train_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_train)\n",
    "\n",
    "# Accuracy with the test data\n",
    "y_pred_test_f = model_11_tuned.predict(X_test_scaled)\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred_test_f))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test_f)))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_test_f))\n",
    "n = len(X_test)\n",
    "p = X_test.shape[1]\n",
    "adj_r2_test = 1 - (1 - r2_score(y_test, y_pred_test_f)) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R2 score:\", adj_r2_test)\n",
    "\n",
    "#Making final prediction\n",
    "y_pred_11 = model_11_tuned.predict(X_scaled) ##\n",
    "\n",
    "##########################################################\n",
    "X['PRS'] = y_pred_11 ####\n",
    "for indexes in null_indexes:\n",
    "    X.loc[indexes,'PRS'] = np.nan\n",
    "y_pred_11= X['PRS'] ####\n",
    "y_pred_11 = y_pred_11.sort_index() ###\n",
    "##########################################################\n",
    "# Saving results\n",
    "df1 = pd.DataFrame(y_test)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = pd.DataFrame(y_pred_test_f,columns=[\"PRS36\"]) ##\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df_test_model_11 = pd.concat([df1, df2], axis=1)##\n",
    "df_test_model_11.to_csv('ANN_Model_11_test.csv',index=False) ##\n",
    "\n",
    "df3 = pd.DataFrame(y_train)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = pd.DataFrame(y_pred_train_f,columns=[\"PRS36\"]) ##\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df_train_model_11 = pd.concat([df3, df4], axis=1) ##\n",
    "df_train_model_11.to_csv('ANN_Model_11_train.csv',index=False) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfO3kv9kc1di"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "Predictors=['Surface Type', 'Material', 'Color', 'Manufacturer', 'Bead Type 1', \n",
    "            'Bead Type 2', 'Traffic', 'Rainfall', 'Snowfall', 'PRS']\n",
    "explainer = shap.DeepExplainer(model_11_tuned, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS9NYaJYc2G9"
   },
   "outputs": [],
   "source": [
    "###\n",
    "feedrow_model_11 = [[1,4,0,12,0,4,16068858,3.95,4.2,predictedoutput_model_10]]\n",
    "feedrow_model_11 = scaler.transform(feedrow_model_11)\n",
    "predictedoutput_model_11= model_11_tuned.predict([feedrow_model_11])\n",
    "print(\"Predicted RS at Month 36:\",predictedoutput_model_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0hGJyYvc_L9"
   },
   "source": [
    "**Model 1 Checking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRjX87e7dDp9"
   },
   "outputs": [],
   "source": [
    "model_1_check = keras.Sequential([\n",
    "    keras.layers.Dense(units=300, activation='relu', input_dim=X_train_scaled.shape[1]),\n",
    "    keras.layers.Dropout(0.0),\n",
    "    keras.layers.Dense(units=1, activation='linear')\n",
    "])\n",
    "model_1_check.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model_1_check.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=16, callbacks=[early_stopping])\n",
    "\n",
    "# Plot the learning curves\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
